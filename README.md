# BenchmarkCycPeptMP
PyTorch implementation for *__An extensive benchmark study on membrane permeability prediction of cyclic peptides__* <br />

Wei Liu, Jianguo Li, Chandra S. Verma and Hwee Kuan Lee*


## ğŸ§¬ Overview

Cyclic peptides are promising drug candidates due to their ability to target proteinâ€“protein interactions. However, their limited membrane permeability remains a major hurdle for intracellular applications. 

In this benchmark study, we systematically evaluate **13 machine learning and deep learning models** for cyclic peptide permeability prediction. These models span **four types of molecular representations**:  
- Fingerprints  
- Strings (e.g., SMILES)  
- Graphs  
- Images  

We test these models across:
- **Three tasks**: regression, binary classification, and soft-label classification  
- **Two data-splitting strategies**: random split and scaffold split  

Our results show that:
- **Graph-based models**, particularly **Directed Message Passing Neural Network (DMPNN)**, consistently achieve top performance.  
- **Regression tasks** generally outperform classification.  
- **Scaffold split**, although better for generalization assessment, leads to lower predictive accuracy than random split.  
- Current models approach the variability of experimental measurements, indicating strong practical value, while still leaving room for further improvement.

---
## ğŸ“¦ Requirements

This project has been tested with the following versions:

- `DeepChem==2.7.1`  
- `RDKit==2022.09.4`  
- `PyTorch==2.0.1`

---

## ğŸ§ª Models Evaluated

### ğŸ“ `ClassicalML/`
- **RF** â€“ Random Forest  
- **SVM** â€“ Support Vector Machine

### ğŸ“ `DeepChemModels/`
- **AttentiveFP**   
- **DMPNN** â€“ Directed Message Passing Neural Network  
- **GAT** â€“ Graph Attention Network  
- **GCN** â€“ Graph Convolutional Network  
- **MPNN** â€“ Message Passing Neural Network  
- **PAGTN** â€“ Path-Augmented Graph Transformer Network  
- **ChemCeption**
### ğŸ“ `PyTorchModels/`
- **RNN** â€“ Recurrent Neural Network  
- **LSTM** â€“ Long Short-Term Memory Network  
- **GRU** â€“ Gated Recurrent Unit

---

## ğŸ“‚ Dataset

We use the **CycPeptMPDB** dataset consisting of over 7,000 curated cyclic peptides with experimentally measured membrane permeability under different assay conditions. For more details, see the [paper](#) and `CSV/` folder.

---

## ğŸš€ How to Train a Model

There are **four ways** to train a model in this repository:

---

### 1. Use `DispatcherMain.py` with `Config.yaml` (**Recommended**)

Edit `Config.yaml` to specify the training configuration, such as:

```yaml
model: RNN
mode: classification
split: random
n_epoch: 1000
```
Them run:
```
python DispatcherMain.py
```
Automatically detects the model type and dispatches to the appropriate main script.


### 2. Run a main script directly with `Config.yaml`

Each main script will read `Config.yaml` as the default:

```bash
python PytorchModels/PytorchModelsMain.py
```
Works the same as DispatcherMain but calls the script explicitly.

### 3. Use `DispatcherMain.py` and override `Config.yaml` via command-line

You can override settings defined in `Config.yaml` by passing command-line arguments:

```bash
python DispatcherMain.py --model GRU --mode regression --split scaffold --n_epoch 2000
```

### 4. Run a main script and override `Config.yaml` via command-line

Same idea as (3), but calls the main script explicitly:

```bash
python DeepChemModels/DeepChemMain.py --model=DMPNN --split=random --mode=soft --batch_size=128
```
